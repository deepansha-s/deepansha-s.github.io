<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/publication_preview/eso-lm.svg" sizes="100%"></source> <img src="/al-folio/assets/img/publication_preview/eso-lm.svg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="eso-lm.svg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="sahoo2025esotericlanguagemodels" class="col-sm-8"> <div class="title">Esoteric Language Models</div> <div class="author"> Subham Sekhar Sahoo, Zhihan Yang, Yash Akhauri, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Johnna Liu, Deepansha Singh, Zhoujun Cheng, Zhengzhong Liu, Eric Xing, John Thickstun, Arash Vahdat' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> 2025 </div> <div class="periodical"> <em><b> I contributed as joint second author. </b></em> </div> <div class="keywords"> <em>Keywords:</em> diffusion language models, language models, generative models</div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.48550/arXiv.2506.01928" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Diffusion-based language models offer a compelling alternative to autoregressive (AR) models by enabling parallel and controllable generation. Among this family of models, Masked Diffusion Models (MDMs) achieve the strongest performance but still underperform AR models in perplexity and lack key inference-time efficiency features—most notably, KV caching. In this work, we introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms, enabling smooth interpolation between their perplexities while overcoming their respective limitations. Eso-LMs set a new state of the art on standard language modeling benchmarks. Crucially, we are the first to introduce KV caching for MDMs while preserving parallel generation, significantly improving inference efficiency. Combined with an optimized sampling schedule, our method achieves up to 65× faster inference than standard MDMs and 4× faster inference than prior semi-autoregressive approaches. We provide the code and model checkpoints on the project page: https://s-sahoo.com/Eso-LMs</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">sahoo2025esotericlanguagemodels</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Esoteric Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sahoo, Subham Sekhar and Yang, Zhihan and Akhauri, Yash and Liu, Johnna and Singh, Deepansha and Cheng, Zhoujun and Liu, Zhengzhong and Xing, Eric and Thickstun, John and Vahdat, Arash}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2506.01928}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2506.01928}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{&lt;em&gt;&lt;b&gt; I contributed as joint second author. &lt;/b&gt;&lt;/em&gt;}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{diffusion language models, language models, generative models}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.48550/arXiv.2506.01928}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="10.1145/3613904.3641998" class="col-sm-8"> <div class="title">ConverSense: An Automated Approach to Assess Patient-Provider Interactions using Social Signals</div> <div class="author"> Manas Satish Bedmutha, Anuujin Tsedenbal, Kelly Tobar, and <span class="more-authors" title="click to view 10 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '10 more authors' ? 'Sarah Borsotto, Kimberly R Sladek, Deepansha Singh, Reggie Casanova-Perez, Emily Bascom, Brian Wood, Janice Sabin, Wanda Pratt, Andrea Hartzler, Nadir Weibel' : '10 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">10 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems</em>, Honolulu, HI, USA, 2024 </div> <div class="periodical"> </div> <div class="keywords"> <em>Keywords:</em> healthcare, interactions, patient-provider communication, social signals</div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3613904.3641998" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Patient-provider communication influences patient health outcomes, and analyzing such communication could help providers identify opportunities for improvement, leading to better care. Interpersonal communication can be assessed through “social-signals” expressed in non-verbal, vocal behaviors like interruptions, turn-taking, and pitch. To automate this assessment, we introduce a machine-learning pipeline that ingests audio-streams of conversations and tracks the magnitude of four social-signals: dominance, interactivity, engagement, and warmth. This pipeline is embedded into ConverSense, a web-application for providers to visualize their communication patterns, both within and across visits. Our user study with 5 clinicians and 10 patient visits demonstrates ConverSense’s potential to provide feedback on communication challenges, as well as the need for this feedback to be contextualized within the specific underlying visit and patient interaction. Through this novel approach that uses data-driven self-reflection, ConverSense can help providers improve their communication with patients to deliver improved quality of care.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3613904.3641998</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bedmutha, Manas Satish and Tsedenbal, Anuujin and Tobar, Kelly and Borsotto, Sarah and Sladek, Kimberly R and Singh, Deepansha and Casanova-Perez, Reggie and Bascom, Emily and Wood, Brian and Sabin, Janice and Pratt, Wanda and Hartzler, Andrea and Weibel, Nadir}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ConverSense: An Automated Approach to Assess Patient-Provider Interactions using Social Signals}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9798400703300}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3613904.3641998}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3613904.3641998}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems}</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{448}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{22}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{healthcare, interactions, patient-provider communication, social signals}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Honolulu, HI, USA}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{CHI '24}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="singh2025workshop" class="col-sm-8"> <div class="title">Flat minima can fail to transfer to downstream tasks</div> <div class="author"> </div> <div class="periodical"> 2023 </div> <div class="periodical"> </div> <div class="periodical howpublished"> <p>Presented at the XYZ Workshop on FooBar, ICML 2025 (non-archival)</p> </div> <div class="keywords"> <em>Keywords:</em> transfer learning, flat minima, loss landscape, deep learning theory, computer vision</div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Large neural networks trained on one task are often finetuned and reused on different but related downstream tasks. The prospect of general principals that might lead to improved transferability is very enticing, as pretraining is exceptionally resource intensive. In recent work, Liu et al. (2022) propose to use flatness as a metric to judge the transferability of pretrained neural networks, based on the observation that, on a suite of benchmarks, flatter minima led to better transfer. Is this a general principal? In this extended abstract, we show that flatness is not a reliable indicator of transferability, despite flatness having been linked to generalization via PAC-Bayes and empirical analysis. We demonstrate that the question of whether flatness helps or hurts depends on the relationship between the source and target tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">singh2025workshop</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Flat minima can fail to transfer to downstream tasks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">howpublished</span> <span class="p">=</span> <span class="s">{Presented at the XYZ Workshop on FooBar, ICML 2025 (non-archival)}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{transfer learning, flat minima, loss landscape, deep learning theory, computer vision}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </body></html>