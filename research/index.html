<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>I started doing CS research in high school after cold-emailing Rutgers faculty, which led to an early graph theory project. Since then, nearly all of my work has focused on machine learning. Along the way I’ve built a strong bias toward open-ended problem solving and grit.<br> <strong>Recognition:</strong> 2023–2024 UC San Diego Physical Sciences Dean’s Undergraduate Award for Excellence (nominated by my thesis advisor).</p> <hr> <h2 id="esoteric-language-models">Esoteric Language Models</h2> <p><strong>Kuleshov Lab, Cornell University (Cornell Tech)</strong> — contributed across several projects:</p> <ul> <li>Studied diffusion and “next-gen” language models (e.g., MDLM, MuLAN); surveyed samplers and schedules.</li> <li>Implemented interpolation between reconstruction and diffusion components at different timesteps.</li> <li>Modified attention-masking mechanisms for a diffusion-transformer denoiser in a targeted use case.</li> <li>Ran Conditional Mauve evaluations; integrated external samplers (e.g., ReMDM) into our codebase.</li> <li>Engineered clean Torch code, ran experiments across configs, analyzed metrics and samples.</li> </ul> <h2 id="google-computer-science-research-mentorship-program-csrmp">Google Computer Science Research Mentorship Program (CSRMP)</h2> <p><strong>Google (Remote)</strong></p> <ul> <li>Mentored by a senior scientist (DeepMind) on core ML topics; worked through selected chapters/resources.</li> <li>Broadened perspective on current research directions and practical workflows.</li> </ul> <h2 id="flat-minima-can-fail-to-transfer-to-downstream-tasks">Flat Minima Can Fail to Transfer to Downstream Tasks</h2> <p><strong>Voluntarily advised by a DeepMind senior research scientist &amp; Vector Institute co-director</strong></p> <ul> <li>Reviewed related work (e.g., Tengyu Ma’s papers) to build foundations.</li> <li>Tested an edge case from prior work; observed generalization/optimization behavior under perturbations.</li> <li>Implemented noise/perturbation pipelines; ran MNIST/CIFAR experiments with ResNets and other models.</li> </ul> <h2 id="conversense-automated-assessment-of-patientprovider-interactions-via-social-signals">ConverSense: Automated Assessment of Patient–Provider Interactions via Social Signals</h2> <p><strong>Weibel Lab, UC San Diego</strong></p> <ul> <li>Surveyed feature-extraction approaches for social-signal detection; tried NVIDIA diarization pipelines.</li> <li>Used <strong>librosa</strong> to compute audio features; inspected importance patterns (e.g., with seaborn).</li> <li>Trained <strong>scikit-learn</strong> classifiers; addressed imbalance with <strong>SMOTE</strong>.</li> <li>Post-submission, explored language-model techniques for healthcare applications.</li> </ul> <h2 id="csnext-program">CSNext Program</h2> <p><strong>University of Washington, Reality Lab (Remote)</strong></p> <ul> <li>Deepened understanding of RNNs/LSTMs; worked with ECG data and classical ML classifiers.</li> <li>Presented final results to the group.</li> </ul> <h2 id="exploring-gradient-free-optimization-algorithms">Exploring Gradient-Free Optimization Algorithms</h2> <p><strong>Dr. Yuhua Zhu Lab (Math Dept Honors Thesis)</strong></p> <ul> <li>Studied federated learning and physics-inspired, gradient-free optimization.</li> <li>Implemented the algorithm (from pseudocode) in PyTorch; debugged and ran on several datasets.</li> <li>Built visualizations of consensus trajectories; parallelized heavy computations for the lab repo.</li> </ul> </body></html>